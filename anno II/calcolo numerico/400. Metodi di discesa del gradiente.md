## Introduzione
Un metodo di discesa consiste in un algoritmo iterativo che, partendo da un certo valore iniziale $x_0 \in \mathbb{R}^n$, genera una successione di vettori $x_0, x_1, x_2, \dots, x_n$ definiti dall'iterazione $$x_{k+1} = x_k + a_kp_k$$dove il vettore $p_k$ è la direzione dove vogliamo muoverci ed $a_k$ è lo step, ovvero la distanza di cui vogliamo muoverci in tale direzione.

> Entrambi questi vettori vanno scelti in modo da garantire la decrescita di $f(x)$ per ogni iterazione.

Per verificare che un vettore $p$ scelto a priori sia una direzione valida per la discesa del gradiente, occorrerà verificare che esista un $\bar{a} : f(x + a \cdot p) \lt f(x), \forall a \in (0, \bar{a}]$, dove $x$ corrisponde al punto dove ci si trova attualmente sul gradiente.

Da cui deriva una proposizione ancora più semplice: se si ha un $\bar{a}$ abbastanza piccolo e la funzione $f : \mathbb{R}^n \rightarrow \mathbb{R}$ è continuamente differenziabile nell'intorno di un punto $x \in \mathbb{R}^n$ (ed ovviamente se $p$ non è un vettore nullo), allora basta verificare che valga $$p^T \nabla f(x) \lt 0$$per verificare che $p$ sia una direzione discendente valida per $f$ in $x$, dato che diminuendo la derivata direzionale in direzione $p$ la funzione $f$ sta decrescendo.

Qualora invece si verificasse che la precedente risulti $\gt 0$, allora la direzione $p$ sarà di salita per $f$ in $x$; per $=0$ invece non se ne potrà essere sicuri.

> Geometricamente parlando, quando si ha $\lt 0$ si forma un angolo ottuso tra il vettore direzione $p$ e $\nabla f(x)$, mentre tra i due se ne formerà uno acuto per $\gt 0$. Questo deriva dalla definizione di coseno in $\mathbb{R}^n$, ponendo: $\cos{\theta} = \frac{x^T y}{||x||||y||}$.

## Scegliere valori opportuni
Scegliere il vettore direzione $p_k$ è molto diretto, in quanto solitamente si cerca di utilizzare $p_k = -\nabla f(x_k)$ per garantire che $p_k^T \cdot \nabla f(x_k) = -\nabla f(x_k) \cdot \nabla f(x_k) \lt 0$ (ovvero che il vettore direzione sia valido).

La scelta del passo risulta invece meno diretta, in quanto comprende l'impiego di apposite tecniche, dette **tecniche di ricerca in linea (line search)** o di **ricerca unidimensionale** perché la ricerca di un nuovo iterato $x_{k + 1}$ avviene lungo la retta $y(a) = x_k + ap_k$.
### La scelta di un passo
Vi sono due modi per scegliere un passo di discesa del gradiente:
- la ricerca esatta (che è costosa computazionalmente);
- la ricerca inesatta (il metodo più pratico);

Solitamente si predilige la ricerca inesatta perché, tramite le condizioni di Wolfe, garantisce una decrescita di $f$ sufficiente per avere una convergenza.

Queste condizioni di Wolfe sono due condizioni che devono essere soddisfatte contemporaneamente per garantire la scelta di un passo $a_k$ adatto. Queste condizioni sono$\dots$ 
#### La condizione di Armijo (sufficient decrease)
$$f(x_k + a \cdot p_k) \leq f(x_k) + c_1 \cdot a \cdot \nabla f(x_k)^T \cdot p_k, \qquad 0 \lt c_1 \lt 1$$
dove il lato sinistro equivale al valore della funzione dopo il passo $a_k$, mentre il lato destro è una linea di riferimento che parte da $f(x_k)$ e scende con una certa pendenza. Essenzialmente, questa condizione dice che ad ogni passo bisogna scendere di almeno una certa distanza, per quanto piccola essa sia.

> Geometricamente questa condizione dice che il nuovo valore deve stare sotto ad una retta con pendenza ridotta ($c_1 \lt 1$ significa che va bene anche una pendenza minore di quella del gradiente reale).
#### La condizione della curvatura
$$\nabla f(x_k) + a_k \cdot p_k^T \cdot p_k \geq c_2 \cdot \nabla f(x_k)^T \cdot p_k, \qquad 0 \lt c_1 \lt c_2 \lt 1$$questa condizione impedisce che il passo $a$ non diventi troppo piccolo, garantendo un passo abbastanza lungo da esplorare lungo la direzione $p$.
#### Algoritmi di Backtracking
Abbiamo visto come per garantire la convergenza della discesa del gradiente occorra soddisfare due condizioni contemporaneamente, quella di [[400. Metodi di discesa del gradiente#Scegliere valori opportuni#La condizione di Armijo (sufficient decrease)|Armijo]] e quella [[400. Metodi di discesa del gradiente#Scegliere valori opportuni#La condizione della curvatura|della curvatura]]. Nel caso in cui però si scelga un passo tramite una tecnica di backtracking, allora sarà sufficiente soddisfare la prima.

Tipicamente:
1. Si parte con $a = 1$ oppure un generico valore iniziale $\bar{a}$
2. Controllo se Armijo è soddisfatta
3. Se **NO**: riduco $a$ di un valore $\theta$ (tipicamente pari a $0.5$) e torno al punto $2$
4. Se **SÌ**: termino e uso questo $a_k$

Un criterio di fallimento di questo algoritmo potrebbe essere la casistica in cui $a$ diventi troppo piccolo, scendendo al di sotto di una soglia prestabilita.

## Schema generale di un metodo con ricerca in linea
```pseudocodice
Input: x_0 (punto iniziale)

Per k = 0, 1, 2, ...
	1. calcola direzione di discesa p_k
	2. trova lunghezza del passo a_k (es. con backtracking)
	3. aggiorna: x_{k+1} = x_k + a_k * p_k
	4. verifica criteri di arresto
```

Dove i criteri di arresto potrebbero essere:
- di successo ($||\nabla f(x_k) || \lt \epsilon$). Il gradiente è quasi zero, quindi ci troviamo vicini ad un punto stazionario;
- di fallimento o limite. Alcuni esempi sono il superamento di un limite di iterazioni, fallimento del backtracking o differenza di step troppo piccola.
## Metodo del gradiente
```pseudocodice
Input: x_0, epsilon, max_iter
k = 0

while ||gradiente(f(x_k))|| > epsilon AND k < max_iter:
	p_k = -gradiente(f(x_k))
	a_k = backtracking(x_k, p_k) // trovo a con Armijo
	x_{k+1} = x_k + a_k * p_k
	k = k + 1
	
return x_k
```
Questo metodo risulta semplice da implementare, ma potrebbe anche essere lento, specialmente quando ci si trova lontani dalla soluzione.
## Metodo di Newton (puro)
Un'alternativa al metodo del gradiente è quella del metodo di Newton, il quale, oltre che alla pendenza (derivata prima) in un determinato punto $x_k$, sfrutta anche la curvatura (derivata seconda) per fare passi più intelligenti.

Qui si usa come direzione la seguente:$$p_k = -H_f(x_k)^{-1}\nabla f(x_k)$$dove $H_f(x_k)$ corrisponde all'Heissiana $\nabla^2f(x_k)$, e la lunghezza di ogni passo $a_k = 1$ per ogni $k$.

Per trovare $p_k$ non inverto quindi $H_f(x_k)$, ma risolvo invece il problema $H_f(x_k)p = -\nabla f(x_k)$.
Questo significa che la direzione $p_k$ sarà in discesa se $H_f(x_k)$ è definita positiva. Nel caso in cui invece sia negativa, si potrebbe non avere una direzione di discesa, fallendo.

I vantaggi di questo metodo sono l'estrema rapidità in prossimità della soluzione, ma il trade-off è la possibilità di non convergere nel caso in cui l'iterato iniziale sia troppo lontano dalla soluzione, oltre che alla elevato costo computazionale ed il rischio di un comportamento sbagliato per funzioni non-convesse.

> Esistono soluzioni che modificano il metodo puro di Newton, molto usate nel campo del Machine Learning.