Un metodo di discesa consiste in un algoritmo iterativo che, partendo da un certo valore iniziale $x_0 \in \mathbb{R}^n$, genera una successione di vettori $x_0, x_1, x_2, \dots, x_n$ definiti dall'iterazione $$x_{k+1} = x_k + a_kp_k$$dove il vettore $p_k$ è la direzione dove vogliamo muoverci ed $a_k$ è lo step, ovvero la distanza di cui vogliamo muoverci in tale direzione.
> Entrambi questi vettori vanno scelti in modo da garantire la decrescita di $f(x)$ per ogni iterazione.

Per verificare che un vettore $p$ scelto a priori sia una direzione valida per la discesa del gradiente, occorrerà verificare che esista un $\bar{a} : f(x + ap) \lt f(x), \forall a \in (0, \bar{a}]$, dove $x$ corrisponde al punto dove ci si trova attualmente sul gradiente.

Da cui deriva una proposizione ancora più semplice: se si ha un $\bar{a}$ abbastanza piccolo e la funzione $f : \mathbb{R}^n \rightarrow \mathbb{R}$ è continuamente differenziabile nell'intorno di un punto $x \in \mathbb{R}^n$ (ed ovviamente se $p$ non è un vettore nullo), allora basta verificare che valga $$p^T \nabla f(x) \lt 0$$per verificare che $p$ sia una direzione discendente valida per $f$ in $x$, dato che diminuendo la derivata direzionale in direzione $p$ la funzione $f$ sta decrescendo.

Qualora invece si verificasse che la precedente risulti $\gt 0$, allora la direzione $p$ sarà di salita per $f$ in $x$; per $=0$ invece non se ne potrà essere sicuri.

> Geometricamente parlando, quando si ha $\lt 0$ si forma un angolo ottuso tra il vettore direzione $p$ e $\nabla f(x)$, mentre tra i due se ne formerà uno acuto per $\gt 0$. Questo deriva dalla definizione di coseno in $\mathbb{R}^n$, ponendo: $\cos{\theta} = \frac{x^T y}{||x||||y||}$.

## Scegliere valori opportuni
Scegliere il vettore direzione $p_k$ è molto diretto, in quanto solitamente si cerca di utilizzare $p_k = -\nabla f(x_k)$ per garantire che $p_k^T \cdot \nabla f(x_k) = -\nabla f(x_k) \cdot \nabla f(x_k) \lt 0$ (ovvero che il vettore direzione sia valido).

La scelta del passo risulta invece meno diretta, in quanto comprende l'impiego di apposite tecniche, dette **tecniche di ricerca in linea (line search)** o di **ricerca unidimensionale** perché la ricerca di un nuovo iterato $x_{k + 1}$ avviene lungo la retta $y(a) = x_k + ap_k$.
### La scelta di un passo
La scelta di un passo $a_k$ non è la causa primaria dei rallentamenti negli algoritmi di discesa del gradiente (quello dipende dalla corretta scelta di una direzione $p$), ma influisce comunque enormemente sul tempo d'esecuzione di questo, con il trade-off del garantirne la convergenza globale
(slide 17)