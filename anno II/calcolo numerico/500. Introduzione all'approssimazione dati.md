## Introduzione
Avendo $n$ coppie di dati nella forma $(x_i, y_i)$ dove $x$ si dice **variabile indipendente** ed y **variabile dipendente**, si dice **modello predittivo** la funzione $f(x)$ che, tramite l'uso di due o più proprietà $\Theta_1, \Theta_2, \dots, \Theta_n$, è capace di predire una $y_k$ partendo da una certa $x_k$.

Per scegliere un modello tipicamente si parte da una famiglia di funzioni (come ad esempio i polinomi di grado $n-$esimo), per poi calcolare i parametri $\Theta$ in modo da determinare quella particolare funzione che meglio rappresenti i dati.

Per capire se una funzione risulti più adatta di un'altra si definisce una **funzione di costo** o **di perdita** $\zeta(\Theta)$ che misuri la distanza tra i dati ed il modello.

> Il vettore ottimale dei parametri $\Theta$ è quello che rende minima la funzione di costo.
## Regressione
La funzione $f$ i cui parametri sono stimati come minimo della funzione di costo si chiama **funzione di regressione**.

### Regressione polinomiale
Nella **regressione polinomiale** la funzione $f$ è un polinomio e la funzione di costo equivale a:$$\zeta(\Theta) = \Sigma_{i=1}^{m}(f(x_i, \Theta) - y_i)^2$$dove $\Theta$ corrisponde al vettore dei coefficienti del polinomio.

Esistono casi particolari di regressione polinomiale:
- la regressione **lineare** ($k = 1$): $f(x) = \Theta_0 + \Theta_1 x$
- la regressione **quadratica** ($k = 2$): $f(x) = \Theta_0 + \Theta_1 x + \Theta_2 x^2$
- il polinomio di grado $k$: dipende da $k + 1$ parametri

La loss function della regressione lineare equivale a:$$\zeta(\Theta) = \Sigma_{i=1}^{m}(f(x_i, \Theta) - y_i)^2$$e misura lo scarto quadratico tra il valore predetto e quello osservato.
## Formulazione matriciale
Dati $m$ punti $(x_i, y_i)$ con $i = 1, \dots, m$ il problema può essere scritto in forma matriciale come:$$\underset{\Theta}{min}||A\Theta - b||_2^2$$Ad esempio la funzione di scarto quadratico della regressione lineare si potrebbe riscrivere come$$\zeta(\Theta) = ||A\Theta - b||_2^2$$