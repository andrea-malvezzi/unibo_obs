1. [[Strutture dati (old)#Liste concatenate|Liste Concatenate]]
	1. [[Strutture dati (old)#Liste Semplici|Liste Semplici]]
	2. [[Strutture dati (old)#Liste Doppiamente Concatenate|Liste Doppiamente Concatenate]]
	3. [[Strutture dati (old)#Altre meno importanti|Altri tipi di Liste Concatenate]]
	4. [[Strutture dati (old)#Operazioni sulle liste|Operazioni sulle Liste Concatenate]]
2. [[Strutture dati (old)#Stack|Stack]]
3. [[Strutture dati (old)#Queue|Queue]]
4. [[Strutture dati (old)#Alberi|Alberi]]
	1. [[Strutture dati (old)#Alberi Binari|Alberi Binari]]
		1. [[Strutture dati (old)#Visitare un Albero Binario|Visitare un Albero Binario]]
	2. [[Strutture dati (old)#Alberi Binari di Ricerca (BST)|Alberi Binari di Ricerca (BST)]]
		1. [[Strutture dati (old)#Nodi Predecessori e Successori in un BST|Nodi Predecessori e Successori in un BST]]
		2. [[Strutture dati (old)#Il Delete nei BST|Operazione di Delete]]
5. [[Strutture dati (old)#Tabelle Hash|Hash Tables]]
	1. [[Strutture dati (old)#Tabelle Hash#Tabelle ad indirizzamento diretto|Tabelle ad indirizzamento diretto]]
	2. [[Strutture dati (old)#Tabelle Hash#Implementazione delle Tabelle Hash|Implementazione delle Tabelle Hash]]
	3. [[Strutture dati (old)#Tabelle Hash#Funzioni di Hash|Funzioni di Hash]]
		1. [[Strutture dati (old)#Tabelle Hash#Funzione di Hashing Metodo della divisione|Metodo della Divisione]]
		2. [[Strutture dati (old)#Tabelle Hash#Funzioni di Hash#Funzione di Hashing Metodo della moltiplicazione|Metodo della Moltiplicazione]]
		3. [[Strutture dati (old)#Tabelle Hash#Funzioni di Hash#Funzione di Hashing Metodo della codifica algebrica|Metodo della Codifica Algebrica]]
	4. [[Strutture dati (old)#Tabelle Hash#Gestione delle Collisioni|Gestione delle Collisioni]]
		1. [[Strutture dati (old)#Tabelle Hash#Gestione delle Collisioni#Concatenamento|Concatenamento]]
			1. [[Strutture dati (old)#Tabelle Hash#Gestione delle Collisioni#Riassunto delle Complessità dei metodi per il Concatenamento|Complessità dei metodi per il Concatenamento]]
		2. [[Strutture dati (old)#Tabelle Hash#Gestione delle Collisioni#Indirizzamento Aperto|Indirizzamento Aperto]]
			1. [[Strutture dati (old)#Tabelle Hash#Ispezione Lineare|Ispezione Lineare]]
			2. [[Strutture dati (old)#Tabelle Hash#Ispezione Quadratica|Ispezione Quadratica]]
			3. [[Strutture dati (old)#Doppio Hashing|Doppio Hashing]]
6. [[Strutture dati (old)#Heap|Heap]]
	1. [[Strutture dati (old)#Array Heap|Array Heap]]
		1. [[Strutture dati (old)#Trovare il valore massimo (o minimo) in un Array Heap|FindMax & FindMin]]
		2. [[Strutture dati (old)#Ripristinare la proprietà di Max-Heap (o di Min-Heap)|FixHeap]]
		3. [[Strutture dati (old)#Cancellare valore massimo (o minimo)|DeleteMax & DeleteMin]]
		4. [[Strutture dati (old)#Costruire un Array Heap a partire da un Array non-ordinato|Heapify]]
	2. [[Strutture dati (old)#Priority Queue|Priority Queue]]
		1. [[Strutture dati (old)#Trovare l'elemento con chiave maggiore (o minore)|FindMax & FindMin]]
		2. [[Strutture dati (old)#Inserire un elemento con una chiave associata|Insert]]
		3. [[Strutture dati (old)#Cancellare un elemento|Delete]]
		4. [[Strutture dati (old)#Priority Queue#Cancellare l'elemento con chiave maggiore (o minore)|DeleteMax & DeleteMin]]
		5. [[Strutture dati (old)#Priority Queue#Aumentare (o diminuire) la chiave di un elemento|IncreaseKey & DecreaseKey]]
	3. [[Strutture dati (old)#d-Heap|d-Heap]]
		1. [[Strutture dati (old)#d-Heap Arrays|d-Heap Arrays]]
7. [[Strutture dati (old)#Strutture Union-Find|Strutture Union-Find]]
	1. [[Strutture dati (old)#Creare un nuovo insieme|MakeSet]]
	2. [[Strutture dati (old)#Cercare l'insieme contenente un certo elemento x|Find]]
	3. [[Strutture dati (old)#Unire due insiemi|Union]]
	4. [[Strutture dati (old)#Esempio di applicazione della Struttura Union-Find|Esempio pratico di applicazione di una Struttura Union-Find]]
	5. [[Strutture dati (old)#Implementazioni delle Strutture Union-find|Possibili implementazioni delle Strutture Union-Find]]
		1. [[Strutture dati (old)#Implementazione QuickFind|QuickFind]]
			1. [[Strutture dati (old)#L'operazione Find con il QuickFind|Find]]
			2. [[Strutture dati (old)#L'operazione Union con il QuickFind|Union]]
		2. [[Strutture dati (old)#Implementazione QuickUnion|QuickUnion]]
			1. [[Strutture dati (old)#L'operazione Find con il QuickUnion|Find]]
			2. [[Strutture dati (old)#L'operazione Union con il QuickUnion|Union]]
		3. [[Strutture dati (old)#QuickFind con Euristica sul peso|QuickFind con Euristica sul Peso]]
			1. [[Strutture dati (old)#Prima Osservazione sul QuickFind con Euristica sul peso|Prima Osservazione]]
			2. [[Strutture dati (old)#Seconda Osservazione sul QuickFind con Euristica sul peso|Seconda Osservazione]]
		4. [[Strutture dati (old)#QuickUnion con Euristica sul peso|QuickUnion con Euristica sul Peso]]
8. [[Strutture dati (old)#Grafi|Grafi]]
	1. [[Strutture dati (old)#Definizioni importanti dei Grafi|Definizioni Importanti]]
	2. [[Strutture dati (old)#Operazioni sui grafi|Operazioni]]
	3. [[Strutture dati (old)#Adiacenza e grafi non orientati - Matrice di Adiacenza|Matrice di Adiacenza nei Grafi non-orientati]]
		1. [[Strutture dati (old)#Operazioni sulle matrici di adiacenza - Grafi non orientati|Operazioni sulle matrici di adiacenza]]
			1. [[Strutture dati (old)#Come mai questi costi?|Spiegazione dei costi delle operazioni]]
	4. [[Strutture dati (old)#Adiacenza e grafi non orientati - Lista di Adiacenza|Lista di Adiacenza nei Grafi non-orientati]]
		1. [[Strutture dati (old)#Operazioni sulle liste di adiacenza - Grafi non orientati|Operazioni sulle liste di adiacenza]]
			1. [[Strutture dati (old)#Operazioni sulle liste di adiacenza - Grafi non orientati#Come mai questi costi?|Spiegazione dei costi delle operazioni]]
	5. [[Strutture dati (old)#Adiacenza nei grafi orientati|Adiacenza nei Grafi orientati]]
	6. [[Strutture dati (old)#Grafi Pesati|Grafi Pesati]]
	7. [[Strutture dati (old)#I Cammini|Cammini]]
		1. [[Strutture dati (old)#Grafi connessi|Grafi connessi]]
		2. [[Strutture dati (old)#Grafi fortemente connessi|Grafi fortemente connessi]]
		3. [[Strutture dati (old)#Grafi debolmente connessi|Grafi debolmente connessi]]
	8. [[Strutture dati (old)#Cicli|Cicli]]
	9. [[Strutture dati (old)#Grafi non orientati completi|Grafi non-orientati completi]]
	10. [[Strutture dati (old)#Curiosità sul rapporto tra Grafi ed Alberi|Il Rapporto tra Grafi ed Alberi]]
	11. [[Strutture dati (old)#Grafi#Visitare i Grafi|Visitare i Grafi]]
		1. [[Strutture dati (old)#Visitare i Grafi#Visita BFS (Breadth First Search)|Visita BFS]]
			1. [[Strutture dati (old)#Visitare i Grafi#Costo computazionale della visita BFS nei Grafi|Costo Computazionale]]
			2. [[Strutture dati (old)#Visitare i Grafi#Alcune applicazioni curiose|Alcune applicazioni curiose]]
		2. [[Strutture dati (old)#Visita DFS (Depth First Search)|Visita DFS]]
			1. [[Strutture dati (old)#Visitare i Grafi#Applicazioni utili della tecnica DFS|Applicazioni utili]]
	12. [[Strutture dati (old)#Grafi#Ordinamento Topologico in un Grafo DAG|Ordinamento Topologico in un Grafo DAG]]
	13. [[Strutture dati (old)#Minimum Spanning Tree|Minimum Spanning Tree]]
		1. [[Strutture dati (old)#Calcolare l'MST|Calcolare l'MST]]
		2. [[Strutture dati (old)#Definizioni di Taglio, Attraversamento e Rispetto del Taglio|Definizioni di Taglio, Attraversamento e Rispetto del Taglio]]
		3. [[Strutture dati (old)#Regola del Taglio|Regola del Taglio]]
		4. [[Strutture dati (old)#Regola del Ciclo|Regola del Ciclo]]
		5. [[Strutture dati (old)#Costruire un MST con le Regole del Taglio e del Ciclo|Costruire un MST con le Regole del Taglio e del Ciclo]]
		6. [[Strutture dati (old)#Esempio di Costruzione di un MST con le Regole del Taglio e del Ciclo|Esempio di Costruzione di un MST con le Regole del Taglio e del Ciclo]]
	14. [[Strutture dati (old)#Algoritmo di Kruskal|Algoritmo di Kruskal]]
		1. [[Strutture dati (old)#Esempio di Applicazione dell'Algoritmo di Kruskal|Applicazione]]
		2. [[Strutture dati (old)#Implementare l'Algoritmo di Kruskal|Implementazione]]
	15. [[Strutture dati (old)#Algoritmo di Prim|Algoritmo di Prim]]
		1. [[Strutture dati (old)#Implementazione dell'Algoritmo di Prim|Implementazione]]
		2. [[Strutture dati (old)#Algoritmo di Prim#Esempio di Applicazione dell'Algoritmo di Prim|Applicazione]]
## Liste concatenate
In questa struttura dati gli elementi sono sequenziali e l'ordine è deciso da una catena di puntatori. Per accedere ad un elemento occorre iterare sulla lista. Ogni lista contiene un puntatore al suo primo elemento, detto $\text{head}$.
Ne esistono $4$ tipi:
### Liste Semplici
Dove ogni nodo $x$ contiene:
- $\text{value}$
- $\text{next} \rightarrow$ il puntatore al prossimo nodo
### Liste Doppiamente Concatenate
Dove ogni nodo contiene, oltre che a $\text{value}$ e $\text{next}$, anche:
- $\text{prev} \rightarrow$ il puntatore al nodo precedente
### Altre meno importanti
Esistono anche liste **concatenate circolari**, ovvero una variante di [[Strutture dati (old)#Liste Doppiamente Concatenate|DLL]] dove il $\text{prev}$ della $\text{head}$ punta alla $\text{tail}$, e le liste con **puntatori alla testa e alla coda**.
### Operazioni sulle liste
| **Operazione**                   | **Caso migliore (primo elemento)** | **Caso medio** | **Caso peggiore (ultimo elemento) |
| -------------------------------- | ---------------------------------- | -------------- | --------------------------------- |
| **Search**                       | $O(1)$                             | $O(n)$         | $O(n)$                            |
| **Delete**                       | $O(1)$                             | $O(n)$         | $O(n)$                            |
| **Insert** (in testa alla lista) | $O(1)$                             |                |                                   |
| **Append**                       | $O(n)$                             |                |                                   |
## Stack
Struttura dati $\text{LIFO}$ che supporta $\text{push}$ e $\text{pop}$. Entrambe le operazioni hanno costo costante.
Si possono poi implementare $\text{IsEmpty}$ e $\text{Top}$, entrambe con costo costante grazie ad una proprietà $\text{size}$ e ad un puntatore all'ultimo elemento inserito.
## Queue
Struttura dati $\text{FIFO}$ che support $\text{Enqueue}$ e $\text{Dequeue}$. Entrambe le operazioni hanno costo costante, grazie ad un puntatore alla $\text{head}$ e alla $\text{tail}$.
## Alberi
Un **albero** è una struttura dati non lineare (non ha una struttura ben definita come le Liste). Un albero è composto da $\text{nodi}$ collegati medianti $\text{archi}$ ed nodo da cui parte tutto l'albero è detto $\text{radice}$.
Un albero possiede:
- una **profondità** associata ad ogni nodo $n$, ovvero la lunghezza del percorso tra $n$ e la radice calcolata in nodi attraversati;
- un **grado** associato ad ogni nodo $n$, ovvero il numero dei suoi figli, nipoti etc $\dots$ ;
- un'**altezza**, ovvero il numero di **livelli** che lo compongono. Un livello corrisponde ad un insieme di nodi ad una certa profondità. I nodi all'ultimo livello si dicono **foglie**;
### Alberi Binari
Un **albero binario** è uno speciale tipo di albero in cui ogni nodo possiede *al massimo* $2$ figli, uno sinistro e uno destro.
Un albero binario si dice:
- **completo** quando ogni suo nodo che non sia una foglia possiede esattamente $2$ figli;
- **perfetto** quando è completo e tutte le sue foglie si trovano al medesimo livello;
- **quasi perfetto** quando è perfetto fino al penultimo livello e le sue foglie sono compattate a sinistra.
#### Visitare un Albero Binario
Un albero si può visitare in 2 modi:
- tramite $\text{DFS - Depth First Search}$
- tramite $\text{BFS - Breadth First Search}$

Nel primo caso si visita ogni nodo ricorsivamente: questo significa che si partirà dai nodi più in basso per poi risalire l'albero piano piano.
In base all'implementazione si può parlare di tre tipi diversi di visita tramite **DFS**:
- $\text{Inorder}:$ visito nodo sinistro $\rightarrow$ nodo centrale (padre) $\rightarrow$ nodo destro;
- $\text{Preorder}:$ visito padre $\rightarrow$ nodo sinistro $\rightarrow$ nodo destro;
- $\text{Postorder}:$ visito nodo sinistro $\rightarrow$ nodo destro $\rightarrow$ padre;

Ognuna di queste implementazioni avrà Costo Computazionale pari ad $O(n)$, dove $n$ equivale al numero di nodi nell'albero studiato.

Nel caso di visita tramite **BFS** invece si visita ogni nodo di un certo livello per poi passare al successivo. Per farlo si usa una [[Strutture dati (old)#Queue|Queue]] nella quale viene inserita prima la radice dell'albero, poi iterativamente tutti i figli dei nodi studiati. In questo modo la visita sarà "sequenziale".
### Alberi Binari di Ricerca (BST)
Un $\text{BST - Binary Search Tree}$ corrisponde ad una variante di Albero Binario in cui un valore minore di quello contenuto dal nodo corrente viene inserito alla sua sinistra, mentre un valore maggiore alla sua destra.
Per trovare il valore minimo o quello massimo di un BST basta quindi seguire rispettivamente i figli di sinistra o di destra, fino a raggiungere una foglia.
#### Nodi Predecessori e Successori in un BST
In un BST si dice successore di un nodo $x$ un altro nodo $k$ con valore **minimo maggiore** di quello di $x$. Questo significa visitare il figlio destro di $x$ e poi visitare sempre quello più a sinistra, fino a raggiungere una foglia. Ad esempio, nell'albero seguente, il successore di $60$ corrisponde a $70$:![[Pasted image 20250607154906.png]]
Per il predecessore si devono invece fare due casi distinti:
- quando il nodo $x$ possiede un sottoalbero sinistro, il suo predecessore sarà il nodo contenente il valore maggiore in tale sottoalbero;![[Pasted image 20250328151743.png]]
- quando il nodo $x$ non possiede un sottoalbero sinistro, il suo predecessore sarà il primo nodo a contenere $x$ nel proprio sottoalbero destro.![[Pasted image 20250328152153.png]]
#### Il Delete nei BST
Quando si desidera rimuovere un nodo $x$ da un BST, occorre pensare a $3$ possibili scenari:
- il nodo $x$ è una foglia: in questo caso non vi sono complicazioni;
- il nodo $x$ ha un solo figlio $k$. Ora $k$ prenderà il posto di $x$. Ciò significa che se $x$ era figlio destro di un altro nodo $g$, allora diventerà il figlio destro di $g$, e viceversa;
- il nodo $x$ ha due figli. In questo caso dovremo cercare il predecessore $p$ di $x$, scambiare i valori di questi due nodi e rimuovere il nodo che era originariamente il predecessore.

Ognuna di queste due operazioni ha costo $O(h)$, dove $h$ equivale all'altezza dell'albero.
## Tabelle Hash
Le $\text{Hash Tables}$ sono delle strutture simili ai Dizionari che implementano le operazioni di $\text{search, insert}$ e $\text{delete}$ con costo costante. Le $\text{chiavi}$ (i valori) che potrebbero essere memorizzati all'interno di una tabella fanno parte di un insieme generale $U$, mentre le chiavi che sono **effettivamente** memorizzate in una tabella fanno parte di un insieme $K$. Ciò significa che mano a mano che si utilizza la tabella, $K$ cambierà in base al numero di valori aggiunti o rimossi da essa, mentre $U$ manterrà la stessa grandezza.
- Se $K \sim U:$ usiamo le [[Strutture dati (old)#Tabelle ad indirizzamento diretto|Tabelle ad indirizzamento diretto]].
- Se $K \ll U:$ usiamo le [[Strutture dati (old)#Implementazione delle Tabelle Hash|Tabelle Hash]].
### Tabelle ad indirizzamento diretto
Consistono in un Array $A$ di grandezza $U$ dove una chiave $k$ viene memorizzata in posizione $k-$esima. Sono efficienti dal punto di vista del **costo temporale**, ma dal punto di vista del **costo spaziale** risultano estremamente inefficienti, in quanto in base alla dimensione di $U$ si potrebbero avere sprechi immensi.
Per ovviare a questo problema si usano le [[Strutture dati (old)#Implementazione delle Tabelle Hash|Tabelle Hash]].
### Implementazione delle Tabelle Hash
Una Tabella Hash consiste in un Array $A$ con dimensione pari ad $m = \Theta(|K|)$, ovvero ad una stima asintotica della cardinalità di $K$. Questo significa che $A$ crescerà e rimpicciolirà durante l'utilizzo.
Ogni chiave dell'insieme $U$ verrà quindi mappata ad un indice $i \in [0, \dots, m]$ tramite una **funzione di hashing** $h$. Così facendo tuttavia potrebbe accadere che due chiavi abbiano lo stesso indice $i$ mappato ad esse, in quanto l'insieme $U$ è più grande di quello $K$. Queste situazioni si chiamano **collisioni** e non sono interamente evitabili, solamente minimizzabili.
### Funzioni di Hash
Una buona funzione di Hash $h$ dovrebbe idealmente fare in modo che ogni chiave dell'insieme $U$ abbia possibilità di essere estratta pari a $\frac{1}{m}$. Questa proprietà è detta di **uniformità semplice**.
Nella realtà questo non è ovviamente possibile in quanto l'insieme $U$ è molto più grande di quello $K$: ci si accontenta quindi di una funzione in grado di limitare e gestire le eventuali collisioni, il tutto in modo efficiente.
#### Funzione di Hashing: Metodo della divisione
La funzione di Hashing basata sul metodo della divisione consiste in una divisione ed ha quindi costo costante. Considerando $m =$ dimensione di $A$:
$$h(x) = x \ mod \ m$$Questo metodo tuttavia provoca collisioni quando:
- $m = 10^a$ oppure $m = 2^a$;
- si ha un $x$ che termina con un $1$;

per limitare la possibilità di collisioni si dovrebbe scegliere un $m$ primo lontano dalle potenze di $10$ e di $2$.
#### Funzione di Hashing: Metodo della moltiplicazione
Anche questa funzione ha costo costante e si implementa come segue:$$h(x) = \lfloor m \cdot(x \cdot C - \lfloor k \cdot C\rfloor)\rfloor$$dove:
- $\lfloor n \rfloor$ indica la parte intera di $n$;
- $C$ indica una costante, che si consiglia di porre pari al reciproco della sezione aurea (ovvero $\frac{\sqrt{5} - 1}{2}$).
#### Funzione di Hashing: Metodo della codifica algebrica
Questo metodo fa la somma di tutti i componenti della rappresentazione binaria di una costante $K$ moltiplicati per l'input $x$, per poi fare il modulo di tale operazione per $m$.$$h(x) = (k_n \cdot x^n + k_{n-1} \cdot x^{n - 1} + \dots + k_0) \ mod \ m$$Questo metodo risulta "lento" in quanto deve effettuare molte operazioni ed ha costo pari a $\cal O(\log_2{k})$.
### Gestione delle Collisioni
Per gestire le collisioni occorre trovare collocazioni alternative per le chiavi collidenti. Ci sono due tecniche efficienti per fare questo:
- $\text{Concatenamento}$ o Scansione Interna;
- $\text{Indirizzamento Aperto}$ o Scansione Esterna.

Entrambi i metodi aumenteranno il costo computazionale delle operazioni dell'Hash Table.
#### Concatenamento
Quando due chiavi $k_1$ e $k_2$ vanno in collisione per un indice $i$, si crea una [[Strutture dati (old)#Liste concatenate|Lista]] contenente tutte queste chiavi e si posiziona in $A[i]$ un puntatore alla testa di tale lista.
Il costo delle operazioni sarà quindi pari ad $\cal O(l)$ nel caso peggiore, dove $l$ equivale alla lunghezza della lista di trabocco più lunga.
```pseudocodice
function Insert(HashTab T, Key k, Data d)
	tmp = llsearch(T[h(k)], k)
	if tmp != NIL 
		tmp.data = d
	else
		llinsert(T[h(k)], k, d)

function Delete(HashTab T, Key k)
	lldelete(T[h(k)], k)

function Search(HashTab T, Key k)
	tmp = llsearch(T[h(k)], k)
	if tmp != NIL
		return tmp.data
	return NIL
```
dove i metodi con prefisso $ll$ operano su una lista concatenata in un determinato indice della Tabella.
##### Riassunto delle Complessità dei metodi per il Concatenamento
| **Metodo**      | **Caso migliore** | **Caso medio** | **Caso peggiore** |
| --------------- | ----------------- | -------------- | ----------------- |
| $\text{Insert}$ | $\cal O(1)$       | $\cal O(1)$    | $\cal O(l)$       |
| $\text{Delete}$ | $\cal O(1)$       | $\cal O(1)$    | $\cal O(l)$       |
| $\text{Search}$ | $\cal O(1)$       | $\cal O(1)$    | $\cal O(l)$       |
#### Indirizzamento Aperto
Supponendo che la tabella possa contenere $\text{NIL}$ quando uno slot non risulta assegnato, una chiave $k$ e $\text{DEL}$ quando un valore è stato cancellato, se due chiavi $k_1$ e $k_2$ collidono per un indice $i$, ispezioniamo la tabella intera per trovare un nuovo spot in cui inserire $k_2$.
Per fare questo sfruttiamo appositi metodi di ispezione:
- $\text{ispezione lineare}$;
- $\text{ispezione quadratica}$;
- $\text{doppio hashing}$.
##### Ispezione Lineare
Qui ci si serve di una funzione di hashing $h'$ usata per calcolare l'Hash di un input $x$ e di un valore $i$ che verrà usato per ispezionare l'indice successivo a quello in cui è successa la collisione:$$h(x, i) = (h'(x) + i) \ mod \ m$$Questo metodo ha un problema: tende a creare sequenze di celle occupate che diventano sempre più lunghe, accrescendo i tempi di inserimento e cancellazione. Ogni cella vuota preceduta da $n$ celle piene avrà probabilità di essere riempito pari a $\frac{n + 1}{m}$.
##### Ispezione Quadratica
Avendo una funzione di hashing ausiliaria $h'$:$$h(x, i) = (h'(k) + c_1 \cdot i + c_2 \cdot i^2) \ mod \ m$$dove $c_1 \not = c_2$.
Qui si procede quindi con passo quadratico per evitare code lunghe e limitare così l'aumento nel costo delle operazioni di $\text{insert}$ e di $\text{delete}$.
##### Doppio Hashing
Qui si hanno due funzioni di hashing ausiliarie, $h'$ e $h''$. Quando si ha una collisione si usa la secondaria assieme all'indice di ispezione per determinare il successivo slot da ispezionare, evitando così i problemi legati agli altri metodi di ispezione.$$h(x, i) = (h_1(x) + i \cdot h_2(x)) \ mod \ m$$dove generalmente si ha $h_1 \not = h_2$.
> **N.B.** la funzione $h_2$ non deve mai ritornare $0$ e deve permettere di iterare su tutta la tabella.

## Heap
L'$\text{Heap}$ è una struttura dati specializzata in operazioni di inserimento e di ricerca di valori massimi o minimi, basata sugli [[Strutture dati (old)#Alberi Binari|Alberi Binari quasi perfetti]].
Un Heap può essere:
- $\text{max-heap}$ quando, per ogni suo nodo $i$ con valore $V[i]$, il genitore di $i$ ha valore $\geq V[i]$;
- $\text{min-heap}$ quando invece che avere valore $\geq V[i]$, il genitore ha valore $\leq V[i]$.

Entrambe le strutture dati condividono le stesse definizioni, in maniera simmetrica.
### Array Heap
Un Heap si può anche rappresentare come un Array $A$ contenente tutti gli elementi contenuti nell'albero associato ad esso.
Chiamando $i$ l'indice nell'array di un certo elemento dell'albero, allora:
- la radice si troverà in $A[0]$;
- il figlio sinistro di $i$ si trova in $2 \cdot i$;
- il figlio destro di $i$ si trova in $2 \cdot i + 1$ (è il vicino del figlio sinistro);
- il genitore di $i$ si trova in $\text{Math.floor}\left(\frac{i}{2}\right)$. Qui l'arrotondamento serve per evitare di fare un caso separato per ogni figlio;
![[Pasted image 20250331095322.png]]
#### Trovare il valore massimo (o minimo) in un Array Heap
La funzione $\text{FindMax}$ (o $\text{FindMin}$) permette di trovare il valore massimo (o minimo) di un Heap: per farlo basterà ritornare la radice dell'albero binario associato, ovvero $A[0]$.
La funzione ha costo costante in quanto si tratta semplicemente di accedere direttamente ad un valore in un array.
#### Ripristinare la proprietà di Max-Heap (o di Min-Heap)
Modificare un Heap potrebbe fare venire meno la proprietà che lo caratterizza, ovvero che per ogni nodo $i$ con valore $V[i]$ si ha:
- $\text{max-heap}$ se il genitore di $i$ ha valore $\geq V[i]$;
- $\text{min-heap}$ se il genitore di $i$ ha valore $\leq V[i]$;

La funzione $\text{FixHeap(i)}$ confronta quindi il nodo $i$ con i suoi figli e, se trova un figlio con valore maggiore (nel caso di un Max-Heap) di quello di $i$, inverte i due per poi richiamare sé stessa sul figlio con cui è stato effettuato lo scambio.
Questa funzione ha costo (nel caso pessimo) pari all'altezza dell'albero, ovvero $O(h)$.
#### Cancellare valore massimo (o minimo)
La funzione $\text{DeleteMax}$ (o $\text{DeleteMin}$) si serve di [[Strutture dati (old)#Ripristinare la proprietà di Max-Heap (o di Min-Heap)|FixHeap]] per eliminare $A[0]$ e poi ripristinare la proprietà originale dell'albero.
Questa operazione ha quindi costo pari a quello di $\text{FixHeap}$, ovvero $O(h)$.
#### Costruire un Array Heap a partire da un Array non-ordinato
La funzione $\text{Heapify}$ necessita della posizione nell'array non-ordinato di quella che diventerà la radice dell'albero e la posizione dell'ultimo elemento da inserire. In seguito dovrà costruire ricorsivamente il sotto-heap destro e quello sinistro della radice, richiamando [[Strutture dati (old)#Ripristinare la proprietà di Max-Heap (o di Min-Heap)|FixHeap]] per riordinare l'Heap creato.
Questa funzione ha quindi costo pari a$$T(n) = \begin{cases}1 & n \leq 1 \\ 2 \cdot T(\frac{n}{2}) + \log{n} & n \gt 1\end{cases}$$che una volta risolto equivale ad $O(n)$.
### Priority Queue
Una $\text{Priority Queue}$ è una struttura dati che ritorna il valore minimo di un insieme di coppie chiave-valore. Si può implementare mediante Heap.
#### Trovare l'elemento con chiave maggiore (o minore)
In un $\text{Max-Heap}$ si ha $\text{FindMin}$, con costo $O(n)$. In un $\text{Min-Heap}$ si ha invece $\text{FindMax}$.
#### Inserire un elemento con una chiave associata
La funzione $\text{Insert}$ aggiunge un elemento $e$ con associata la chiave $k$ alla Priority Queue. L'operazione non fa altro che aggiungere un elemento all'ultimo spazio vuoto dell'array per poi chiamare [[Strutture dati (old)#Ripristinare la proprietà di Max-Heap (o di Min-Heap)|FixHeap]] e risalire l'albero. L'operazione ha quindi costo nel caso peggiore $O(n)$ (deve iterare su tutti gli elementi dell'array), altrimenti $O(\log{n})$ (deve risalire l'albero con FixHeap).
#### Cancellare un elemento
La funzione $\text{Delete}$ rimuove un elemento dalla coda, supponendo di avere accesso diretto a tale elemento $e$. Questo permette di avere un accesso a costo costante.
Tuttavia dopo aver eliminato $e$ occorrerà chiamare [[Strutture dati (old)#Ripristinare la proprietà di Max-Heap (o di Min-Heap)|FixHeap]] per sistemare l'Heap: l'operazione avrà quindi costo pari a $O(\log{n})$.
#### Cancellare l'elemento con chiave maggiore (o minore)
La funzione $\text{DeleteMax}$ (e simmetricamente $\text{DeleteMin}$) non fa altro che chiamare [[Strutture dati (old)#Trovare l'elemento con chiave maggiore (o minore)|FindMax]] per poi eliminare tale elemento e sistemare l'Heap con [[Strutture dati (old)#Ripristinare la proprietà di Max-Heap (o di Min-Heap)|FixHeap]].
L'operazione avrà quindi costo pari a $O(n)$ nel caso peggiore (chiamo DeleteMax in un Min-Heap) e $O(\log{n})$ nel caso migliore & medio (in un Max-Heap l'elemento maggiore è sempre la radice: il costo è dettato da FixHeap).
#### Aumentare (o diminuire) la chiave di un elemento
La funzione $\text{IncreaseKey}$ (e simmetricamente $\text{DecreaseKey}$) aumenta la chiave di un elemento $e$ (supponendo di aver accesso diretto a questo). In seguito occorrerà chiamare [[Strutture dati (old)#Ripristinare la proprietà di Max-Heap (o di Min-Heap)|FixHeap]]: il costo è quindi dettato da quest'ultima funzione.
### d-Heap
I d-Heap sono un'estensione dei Max/Min-Heap dove ogni nodo può avere grado pari ad $k$. Un d-Heap avrà quindi altezza pari a $h = \log_k{n}$ dove $n$ equivale al numero di nodi nell'albero.
#### d-Heap Arrays
Un d-Heap si può rappresentare come un Array $A$ dove, dato un indice $i$ equivalente ad un nodo dell'albero associato, valgono le seguenti:
- l'ultimo figlio di $i$ si trova in $\left(i \cdot k\right) + 1$;
- il primo figlio di $i$ si trova in $((i - 1) \cdot k) + 2$, cioè $k - 1$ posizioni prima dell'ultimo figlio;
- il padre di $i$ si trova in $\text{Math.Floor}(\frac{i - 1}{k})$.
## Strutture Union-Find
Le $\text{Strutture Union-Find}$ sono utili per gestire insiemi disgiunti, ovvero **senza elementi in comune**, come gli insiemi singoletto.
Una struttura Union-Find è una collezione $S = \{S_1, \dots, S_k\}$ di insiemi disgiunti, ognuno con un **rappresentante univoco**.
Complessivamente la struttura Union-Find può contenere $n \geq k$ elementi.
### Creare un nuovo insieme
La funzione $\text{MakeSet}$ crea un nuovo insieme a partire da un elemento $x$ (che ne diventa quindi il **rappresentante**). Tuttavia $x$ non deve appartenere già ad altri insiemi della struttura, in quanto essa per definizione opera su insiemi disgiunti.
### Cercare l'insieme contenente un certo elemento x
La funzione $\text{find}$ cerca l'insieme contenente l'elemento $x$ e ne ritorna il rappresentate.
### Unire due insiemi
La funzione $\text{Union}(x, y)$ unisce gli insiemi rappresentati da $x$ e da $y$. Questa operazione cancellerà i due insiemi uniti e creerà un nuovo insieme con un nuovo rappresentate.
### Esempio di applicazione della Struttura Union-Find
Supponiamo di avere $9$ strade e di volerne trovare le intersezioni:
![[Pasted image 20250403122500.png]]
allora potremmo scrivere l'insieme di partenza $E$ come l'insieme di tutte e $9$ le strade (che sono quindi dei singoletti):$$E = \{\{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\}, \{7\}, \{8\}, \{9\}\}$$poi per unire tra loro le strade come da disegno, usiamo la funzione [[Strutture dati (old)#Unire due insiemi|Union]]:
- $\text{Union}(1, 4) \rightarrow \{1, 4\}$;
- $\text{Union}(2, 5) \rightarrow \{2, 5\}$;
- $\text{Union}(1, 2) \rightarrow \{1, 2, 4, 5\}$;
- $\text{Union}(1, 7) \rightarrow \{1, 2, 4, 5, 7\}$;
- $\text{Union}(3, 6) \rightarrow \{3, 6\}$;
- $\text{Union}(8, 9) \rightarrow \{8, 9\}$;
- $\text{Union}(3, 8) \rightarrow \{3, 6, 8, 9\}$.
### Implementazioni delle Strutture Union-find
#### Implementazione QuickFind
Ogni insieme viene rappresentato con un albero di altezza uno, dove la radice corrisponde al rappresentante e i figli della radice sono **tutti** gli elementi dell'insieme, rappresentante incluso. Ovvero:![[Pasted image 20250403125155.png]]come possibile notare dalle frecce nel diagramma, si ha accesso diretto alle foglie ma non alla radice. Ciò significa che da una foglia (ad esempio da $c$) potrò risalire alla radice (ad esempio $b$).
##### L'operazione Find con il QuickFind
L'accesso agli elementi avviene direttamente e ha quindi costo costante.
##### L'operazione Union con il QuickFind
Nell'implementazione $\text{QuickFind}$ ogni elemento è collegato da un puntatore alla radice: ciò significa che unire due insiemi comporterà un cambio di $n$ puntatori, dove $n$ corrisponde al numero complessivo elementi nei due alberi da unire.![[Pasted image 20250403131449.png]]
#### Implementazione QuickUnion
Si rappresenta ogni insieme tramite un albero dove ogni nodo ha esattamente un padre, ma può avere $n$ figli. Anche in questo caso la radice contiene il rappresentante. Ovvero:![[Pasted image 20250403130827.png]]
##### L'operazione Find con il QuickUnion
Il **Find** in questo caso risulta più lento rispetto a [[Strutture dati (brutta)#L'operazione Find con il QuickFind|quello del QuickFind]] perché per trovare la radice occorre passare attraverso tutti i padri di un elemento passato come parametro alla funzione.
##### L'operazione Union con il QuickUnion
Per la Union basterà collegare il padre di un sottoalbero diretto della radice al nuovo rappresentante: si cambierà ora solamente un elemento invece che $n$).![[Pasted image 20250403131420.png]]
#### QuickFind con Euristica sul peso
Una strategia per diminuire il costo dell’operazione [[Strutture dati (brutta)#L'operazione Union con il QuickFind|union]] nel QuickFind consiste nel:
- memorizzare nella radice il numero di elementi dell'insieme; la dimensione può essere mantenuta in tempo $O(1)$;
- appendere l'insieme con meno elementi a quello con più elementi.

Ad esempio, avendo 2 insiemi della forma:![[Pasted image 20250403180816.png]]
con l'algoritmo QuickFind con euristica sul peso si muoverebbero gli elementi $f$ e $g$ dal primo insieme al secondo, nella maniera seguente:![[Pasted image 20250407132819.png]]

##### Prima Osservazione sul QuickFind con Euristica sul peso
Ogni volta che una foglia cambia padre, fa parte di un insieme contenente *almeno* il doppio degli elementi contenuti nel suo insieme originale: ciò significa che ogni foglia cambia il proprio padre al più $\log{n}$ volte.
##### Seconda Osservazione sul QuickFind con Euristica sul peso
Nel caso peggiore fino a $\frac{n}{2}$ elementi cambieranno padre per esecuzione di union, dove $n$ equivale al numero di elementi complessivo dei due insiemi.
Nel caso medio, considerando $k - 1$ cambi (per $k$ cambi si ha una nuova struttura Union-Find) e ricordando che un cambio ha costo pari a $\log{k}$, il costo ammortizzato della funzione union risulta essere $O(k \cdot \log{k})$ diviso il numero di cambi, ovvero $\frac{O(k \cdot \log{k})}{k - 1}$, che risulta $O(\log{k})$ (il $-1$ si può ignorare in quanto non influisce sul calcolo).
#### QuickUnion con Euristica sul peso
%% TODO: completa qui %%
## Grafi
Un $\text{Grafo}$ corrisponde ad una **coppia di vertici** $(V, E)$ e può essere **orientato** o **non-orientato**:
- nei grafi orientati si ha una **relazione binaria** tra i vertici (un senso di marcia) e vi possono essere **cappi**, ovvero archi tra un vertice e sé stesso;
- nei grafi non orientati si hanno coppie non ordinate e non possono esservi cappi.

Il grafo ottenuto ignorando la direzione degli archi e la presenza dei cappi nei grafi orientati si dice **versione non-orientata** di quello specifico Grafo.
![[Pasted image 20250528164918.png]]
<div style="text-align: center;">Esempio di grafo orientato</div>

![[Pasted image 20250528165244.png]]<div style="text-align: center;">Esempio di grafo non orientato</div>
### Definizioni importanti dei Grafi
In un grafo orientato un **arco** $\left(v,w\right)$ si dice **incidente** da $v$ a $w$. Inoltre un **vertice** $w$ si dice **adiacente** ad un altro vertice $v$ quando da $v$ si può proseguire seguendo il verso imposto dal grafo e raggiungere immediatamente $w$.
> **N.B.** In un Grafo non-orientato, l'adiacenza tra vertici è simmetrica (se $w$ è adiacente a $v$, allora anche $v$ è adiacente a $w$). Questo non si può invece dire per i Grafi orientati.

Ricordando l'esempio di Grafo orientato proposto in precedenza, si avrebbe che (ad esempio) $\text{A}$ e $\text{D}$ siano adiacenti tra loro, mentre $\text{B}$ sia adiacente ad $\text{A}$, ma non il contrario.
Infine in un Grafo si può parlare di **grado**:
- in un Grafo non-orientato, il grado di un vertice corrisponde agli archi di cui esso fa parte;
- in un Grafo Orientato si hanno $3$ tipi diversi di grado:
	- si ha un **grado uscente**: il numero di archi incidenti partenti da un certo vertice;
	- un **grado entrante**: il numero di archi incidenti che partono da vertici adiacenti a quello studiato, di cui esso fa parte;
	- un grado **complessivo**: la somma tra grado entrante e grado uscente.
### Operazioni sui grafi
Un grafo (di qualunque tipo esso sia) deve supportare le seguenti operazioni:
- $\text{grado(vertice v)} \rightarrow \text{intero}$
- $\text{archiIncidenti(vertice v)} \rightarrow (\text{arco, arco, }\dots)$
- $\text{estremi(arco e)} \rightarrow (\text{vertice, vertice})$
- $\text{sonoAdiacenti(vertice x, vertice y)} \rightarrow \text{booleano}$
- $\text{aggiungiVertice(vertice v)}$
- $\text{aggiungiArco(vertice x, vertice y)}$
- $\text{rimuoviVertice(vertice v)}$
- $\text{rimuoviArco(arco e)}$
Ognuna di queste operazioni avrà un certo costo computazionale basato sulla tipologia di grafo per cui vengono implementate.
### Adiacenza e grafi non orientati - Matrice di Adiacenza
Per tenere traccia delle adiacenze in un grafo non orientato si può usare una matrice detta **di adiacenza**: questa corrisponde ad una matrice di booleani dove si ha $1$ quando due grafi sono adiacenti e $0$ quando non lo sono:![[Pasted image 20250529135422.png]]
#### Operazioni sulle matrici di adiacenza - Grafi non orientati
| Operazione                                                       | Costo    |
| ---------------------------------------------------------------- | -------- |
| $\text{grado}$(vertice v) $\rightarrow \text{int}$               | $O(n)$   |
| $\text{archiIncidenti}$(vertice v) $\rightarrow$ (arco, arco, …) | $O(n)$   |
| $\text{sonoAdiacenti}$(x, y) $\rightarrow$ $\text{booleano}$     | $O(1)$   |
| $\text{aggiungiVertice}$(v)                                      | $O(n^2)$ |
| $\text{aggiungiArco}$(x, y)                                      | $O(1)$   |
| $\text{rimuoviVertice}$(v)                                       | $O(n^2)$ |
| $\text{rimuoviArco}$(e)                                          | $O(1)$   |
##### Come mai questi costi?
- L'aggiunta o la rimozione di un vertice corrisponde ad $n^2$ cambiamenti, dove $n$ sarebbe la nuova lunghezza di una riga della matrice. Ad esempio, eliminando il vertice $F$ dalla matrice precedentemente usata come esempio, occorrerebbe rimuovere un'intera riga ed un'intera colonna di "paragoni";
- un arco corrisponde al nesso tra due vertici, ovvero ad una cella della matrice, identificata dall'indice del vertice $x$ e del vertice $y$. Aggiungere o rimuovere archi ha quindi costo costante;
- cercare di identificare con quali altri vertici un certo vertice compone un arco significa iterare su un'intera riga o un'intera colonna. L'operazione ha quindi costo pari ad $n$;
### Adiacenza e grafi non orientati - Lista di Adiacenza
Un altro modo per rappresentare e tenere traccia delle adiacenze in un grafo non orientato corrisponde alla Lista di Adiacenza: per ogni vertice si ha una Linked List dove ogni nodo corrisponde ad un vertice adiacente.![[Pasted image 20250529144358.png]]
#### Operazioni sulle liste di adiacenza - Grafi non orientati
| **Operazione**                                                   | **Costo**                                    |
| ---------------------------------------------------------------- | -------------------------------------------- |
| $\text{grado}$(vertice v) $\rightarrow \text{int}$               | $\mathcal{O}(\delta(v))$                     |
| $\text{archiIncidenti}$(vertice v) $\rightarrow$ (arco, arco, …) | $\mathcal{O}(\delta(v))$                     |
| $\text{sonoAdiacenti}$(x, y) $\rightarrow  \text{booleano}$      | $\mathcal{O}(\min\{\delta(x),\ \delta(y)\})$ |
| $\text{aggiungiVertice}$(vertice v)                              | $\mathcal{O}(1)$                             |
| $\text{aggiungiArco}$(vertice x, vertice y)                      | $\mathcal{O}(1)$                             |
| $\text{rimuoviVertice}$(vertice v)                               | $\mathcal{O}(m)$                             |
| $\text{rimuoviArco}$(arco e)                                     | $\mathcal{O}(\delta(x) + \delta(y))$         |
dove:
- $\delta$ corrisponde al grado di un certo vertice;
- $m$ corrisponde al numero di archi nel grafo.
##### Come mai questi costi?
- Trovare gli archi incidenti o il grado di un vertice significa iterare dall'inizio della lista associata a tale vertice fino al termine di essa, eseguendo un numero di operazioni pari al grado stesso del vertice;
- capire se due vertici sono adiacenti significa iterare lungo la lista più corta associata ad uno dei due vertici studiati;
- aggiungere un vertice significa creare una nuova lista;
- aggiungere un arco significa aggiungere un nuovo nodo nella lista associata ad entrambi i vertici facenti parte dell'arco. Lo spostamento a destra del resto della lista ha costo ignorabile;
- rimuovere un vertice significa far scorrere tutte le liste ed eliminare eventuali nodi contenenti tale vertice. Questa è potenzialmente l'operazione più costosa tra tutte, avendo come costo il numero di archi del grafo;
- rimuovere un arco significa rimuovere due nodi, uno per ogni lista associata ad uno dei vettori dell'arco. Questo può potenzialmente richiedere lo scorrimento di entrambe le liste associate a tali vertici.
### Adiacenza nei grafi orientati
Anche nei grafi orientati si possono adoperare sia le Matrici che le Liste di Adiacenza: le uniche differenze si avranno nel fatto che qui cambierà il concetto di adiacenza:![[Pasted image 20250529151001.png]]Qui si può notare come, osservando la riga inerente al vertice $A$, si abbia un $1$ nell'incontro con $D$ e $B$, mentre nella riga inerente a $B$ si abbia un $1$ solamente nell'incontro con il vertice $C$ (e non anche con $A$, come accadeva nei grafi non orientati). La stessa cosa avverrà anche con le Lista di Adiacenza.
### Grafi Pesati
In questa variante dei grafi ad ogni arco viene associato un **peso** (o **costo**) e quando tra due vertici non vi è un arco si dice che i due hanno tra loro **costo infinito**. A seguire un esempio di Matrice di Adiacenza inerente ad un grafo non orientato pesato:![[Pasted image 20250529160754.png]]
### I Cammini
Si dice **cammino** una sequenza di vertici $\lt v_0, v_1, \dots, v_n \gt$ adiacenti tra loro. La **lunghezza** di un cammino corrisponde al numero di archi attraversati (quindi al numero di vertici $-1$).
Un cammino si dice inoltre **semplice** se ogni vertice compare una sola volta all'interno di esso.
A seguire un esempio di cammino semplice:![[Pasted image 20250529161836.png]]
Qualora esistesse un cammino che colleghi due vertici $x$ ed $y$, allora $y$ si direbbe **raggiungibile** da $x$ tramite un certo cammino $c$.
#### Grafi connessi
Un grafo non orientato si dice connesso qualora esista un cammino da ogni vertice ad ogni altro vertice. A seguire un esempio di grafo connesso:![[Pasted image 20250529162230.png]]ed uno di grafo non connesso:![[Pasted image 20250529162304.png]]
#### Grafi fortemente connessi
Un grafo orientato si dice fortemente connesso qualora esista un cammino da ogni vertice ad ogni altro vertice. Tuttavia questo risulta più complicato che nei grafi non orientati, in quanto si deve tenere conto anche dei versi dei vertici: il seguente grafo non risulterà difatti fortemente connesso, in quanto non esisterà un cammino per collegare un qualunque vertice ad $A$.![[Pasted image 20250529162737.png]]
#### Grafi debolmente connessi
Avendo un grafo orientato non fortemente connesso la cui controparte non orientata risulta connessa, allora il grafo orientato è detto **debolmente connesso**. Riproponendo l'esempio di grafo non fortemente connesso presentato in precedenza:
![[Pasted image 20250529163244.png]]
questo sarebbe un valido grafo debolmente connesso.
### Cicli
Un ciclo corrisponde ad un cammino con lunghezza:
- $\geq 1$ nei grafi orientati;
- $\geq 3$ nei grafi non orientati;
dove il primo vertice corrisponde all'ultimo, ovvero ad una situazione del tipo $\lt v_0, v_1, \dots, v_n \gt$ con $v_0 = v_n$.
Un ciclo si dice **semplice** se tutti i vertici facenti parte di esso, meno ovviamente il primo, sono distinti.
Un grafo non orientato si dice **aciclico** quando non contiene cicli semplici, mentre un grafo orientato risulta **aciclico** quando non contiene cicli in generale.
### Grafi non orientati completi
Un grafo non orientato si dice **completo** quando ha un arco tra ogni coppia di vertici, come nel caso seguente:![[Pasted image 20250529164242.png]]In un grafo completo vi sono $\frac{n(n-1)}{2}$ archi totali, dove $n$ corrisponde al numero di vertici.
### Curiosità sul rapporto tra Grafi ed Alberi
La struttura dati dell'albero corrisponde ad un grafo dove un vertice viene eletto a **radice**, creando così un **albero radicato**. Esistono anche **alberi liberi**, dove non vi è una radice. Questi alberi corrispondono a dei grafi non orientati aciclici:![[Pasted image 20250529164632.png]]
### Visitare i Grafi
Dato un Grafo $G = \lt V, E \gt$ ed un vertice $s$ appartenente a tale Grafo, detto **vertice sorgente**, occorre visitare tutti i nodi di $G$ raggiungibili da $s$, visitando ogni nodo una sola volta.
Per fare ciò occorre tenere traccia del fatto che un vertice sia stato visitato o meno, mediante tre status:
- **inesplorato**: l'algoritmo non è ancora giunto a tale nodo;
- **aperto**: non tutti gli archi adiacenti sono stati visitati;
- **chiuso**: tutti gli archi adiacenti sono stati visitati.
#### Visita BFS (Breadth First Search)
La tecnica di visita $\text{BFS}$ nei grafi visita i nodi distanti $k$ dalla sorgente prima di visitare i nodi distanti $k + 1$, generando cosi un albero dove la distanza tra un nodo e la radice (la sorgente) corrisponde alla distanza minima tra i due all'interno di $G$. Questo algoritmo non visita i nodi separati.
```pseudocodice
function BFS(Grafo G, Vertice s) -> Tree
	for Vertice v in G
		v.mark = false // inesplorato
	Tree T = s
	let Q be a new Queue
	Enqueue(Q, s)
	s.mark = true
	s.dist = 0 // tengo traccia della distanza, da settare poi nei nodi di T
	while Q.size != 0
		q = Dequeue(Q)
		for v adiacente a q
			if !v.mark // se il nodo adiacente non risulta marchiato in prec
				Insert(T, v) // aggiungo v all'albero
				Enqueue(Q, v)
				v.parent = q // il genitore corrisponde a q (serve appena sotto)
				v.dist = q.dist + 1 // distanza del genitore + 1
	return T
```
Quindi essenzialmente marca tutti i vertici di $G$ come $\text{false}$, crea un albero con radice $s$, aggiunge $s$ ad una coda e fino a quando non si hanno più elementi nella coda continua ad aggiungervi nodi adiacenti a quello studiato attualmente (a meno che questi non siano marchiati $\text{true}$) e aumenta per ognuno la distanza, sulla base della distanza del genitore.
##### Costo computazionale della visita BFS nei Grafi
Qui si itera sempre su tutti i vertici di $G$, per marchiarli come $\text{false}$. Inoltre si ha un ciclo che fa passare tutte le adiacenze, mediante la $\text{Queue}$. Supponendo di avere quindi $n$ vertici ed un totale di $m$ adiacenze, il costo computazionale dipenderà da quale dei due prevarrà sull'altro, ovvero: $\cal O(\text{max} \{n, m\})$.
##### Alcune applicazioni curiose
Questa tecnica risulta molto efficace per trovare il percorso minimo tra un vertice e tutti i nodi raggiungibili da esso: questo potrebbe significare ad esempio trovare l'uscita di un labirinto partendo da un generico punto (ogni bivio corrisponde ad un vertice).
#### Visita DFS (Depth First Search)
Questa tecnica, come la $\text{BFS}$, marchia i vertici visitati per differenziarli da quelli non visitati, ma punta a restituire una **foresta** invece che un albero, in quanto visiterà tutti i nodi presenti in $G$.
```pseudocodice
global int time = 0 // variabile globale

function DFS(Grafo G)
	for Vertice v in G
		v.mark = "white" // da visitare
		v.parent = NIL
	for Vertive v in G
		if v.mark == "white"
			DFSVisit(v)

function DFSVisit(Vertice v)
	v.mark = "gray" // sto visitando
	time += 1
	v.dt = time // tempo di inizio visita
	for Vertice u adiacente a V
		if u.mark == "white"
			u.parent = v
			DFSVisit(u)
	time += 1
	v.ft = time // tempo per fine visita (tutti i vicini sono stati visitati)
	v.mark = "black" // done
```
Essenzialmente partendo da un Grafo $G$ marchiamo tutti i suoi vertici come da esplorare, poi visitiamo tutti quelli marchiati in questa maniera (il check risulta importante dato che la chiamata ricorsiva sotto visita tutti i vicini ogni volta. Questo abbassa il costo e visita ogni nodo una sola volta).
La funzione $\text{DFSVisit}$ invece marchia un vertice come "in visita" per poi assegnare un **timestamp** di inizio e di fine visita a tale.
Questo **timestamp** ci permette di capire se un vertice $v$ risulta discendente di un altro vertice $u$: questo difatti accade quando si ha $$u.dt \lt v.dt \lt u.ft \lt v.ft$$Inoltre questa misura temporale permette di ricavare il verso di un arco all'interno dei grafi orientati:
- se $v.dt \lt u.dt$ e $u.ft \lt v.ft$ allora l'arco $(u, v)$ è all'indietro;
- se $u.dt \lt v.dt$ e $v.ft \lt u.ft$ allora l'arco $(u, v)$ è in avanti;
- se $v.ft \lt u.dt$ l'arco $(u, v)$ è di attraversamento (collega due nodi in alberi diversi della foresta $\text{DFS}$).
##### Applicazioni utili della tecnica DFS
Alcune applicazioni della tecnica $\text{DFS}$ consistono nel verificare il $\text{DAG}$, ovvero il $\text{Direct Acyclic Graph}$ (un Grafo orientato senza cicli), oppure individuare le componenti connesse di un grafo non orientato.
L'applicazione più utile risulta tuttavia essere l'[[Strutture dati (old)#Ordinamento Topologico in un Grafo DAG|ordinamento topologico in un Grafo DAG]].
### Ordinamento Topologico in un Grafo DAG
Dato un $\text{DAG}$ identificato con la lettera $G$, un ordinamento topologico di $G$ è un **ordinamento lineare** dei suoi vertici per cui se $G$ contiene un arco $(u, v)$ allora $u$ compare prima di $v$ nell'ordinamento.
> **N.B.** Possono esistere più ordinamenti topologici per uno stesso grafo $G$.

Effettuare un ordinamento topologico risulta molto semplice: basta eseguire una visita $\text{DFS}$ sul grafo $G$ con una piccola modifica: quando un nodo viene marchiato di nero (ovvero quando lo si ha visitato), lo si inserisce in una Lista. Al termine basta ritornare la Lista in ordine decrescente per avere uno dei possibili ordinamenti topologici.
Ad esempio, il grafo seguente:![[Pasted image 20250612152454.png]]ritornerebbe la Lista $L = [1, 3, 5, 2, 4]$.
### Minimum Spanning Tree
Un problema ricorrente nei Grafi consiste nel come connettere diversi elementi minimizzando il percorso tra essi. Questo problema prende il nome di $\text{Minimum Spanning Tree}$.
Per calcolare questo si usa una **funzione peso** $w : V \times V \rightarrow \mathbb{R}$ applicata ad un Grafo non-orientato $G = \lt V, E \gt$ (dove $V =$ insieme dei vertici di $G$; $E =$ insieme degli archi di $G$).
Un **albero di copertura** di $G$ corrisponde quindi ad un sotto-grafo $T=  \lt V, E_T \gt$ dove $T$ è un albero contenente tutti i vertici di $G$.
Tra tutti i possibili alberi di copertura, quello con peso minimo corrisponde al $\text{Minimum Spanning Tree}$.
#### Calcolare l'MST
L'idea sarebbe quella di accrescere $T$ piano piano aggiungendo archi sicuri all'insieme. Un arco sicuro sarebbe un arco $\{v ,e\}$ per cui $T \cup \{v,e\}$ è ancora un sottoinsieme di qualche $\text{MST}$.
Generalmente si fa quindi questo per costruire l'$\text{MST}$:
```pseudocodice
function GenericMST(Grafo G, function w){
	let T be a new Tree
	while T non forma un MST
			trova un arco sicuro {u, v}
			T = T U {u, v}
	return T
}
```
#### Definizioni di Taglio, Attraversamento e Rispetto del Taglio
Un **Taglio** corrisponde ad una partizione dell'insieme $V$ in due sottoinsiemi disgiunti.
Avendo un taglio ($S, V - S$), dove $S$ corrisponde al sottoinsieme da rimuovere da $V$ per effettuare la divisione. Si dice che un arco $\{v, u\}$ **attraversa** il taglio se $v \in S$ e $u \in V - S$ e si dice **leggero** se il suo peso risulta minimo tra i pesi degli archi attraversanti. Un taglio **rispetta** un insieme di archi $T$ se nessun arco di $T$ attraversa il taglio.
Definiamo come **archi blu** gli archi facenti parte dell'$\text{MST}$ e come **rossi** quelli non facenti parte di quest'ultimo.
#### Regola del Taglio
Scegliendo un taglio $C$ in $G$ che **rispetti** gli archi blu (e quindi che non sia attraversato da essi), selezionare un arco leggero da cui $C$ sia attraversato (di nessun colore) e inserirlo nell'$\text{MST}$.
#### Regola del Ciclo
Scegli un ciclo semplice in $G$ che non contenga archi rossi. Tra tutti gli archi senza colore del ciclo, selezionare quello con peso massimo e colorarlo di rosso.
#### Costruire un MST con le Regole del Taglio e del Ciclo
Per costruire un $\text{MST}$ con le due regole precedenti basterà applicarle in successione (una vale l'altra, purché la si possa utilizzare).
##### Esempio di Costruzione di un MST con le Regole del Taglio e del Ciclo
Avendo il seguente grafo non-orientato $G$:
![[Pasted image 20250614110418.png]]
Potremmo procedere con la regola del Taglio o con quella del Ciclo, a preferenza personale. Per l'esempio useremo quella del Taglio. Quest'ultima ha come unica restrizione il dover rispettare eventuali archi blu, che però sono qui assenti. Tagliamo quindi a piacere in $G$:![[Pasted image 20250614110903.png]]
A questo punto terminiamo l'applicazione della regola del Taglio selezionando l'arco con peso minimo tra quelli attraversanti il taglio effettuato: in questo caso, l'insieme dei tagli attraversanti sarebbe: $\{\{h, g\}, \{i, g\}, \{c, f\}, \{c, d\}\}$, dove $\{h, g\}$ ha peso $1$. Coloriamo quindi questo di blu.
Procediamo ora applicando una tra le due regole. In questo caso procederemo con quella del ciclo (ma avremmo potuto benissimo usare anche quella del Taglio).
Selezioniamo un ciclo qualunque e coloriamo di rosso l'arco con peso massimo all'interno di esso:
![[Pasted image 20250614111241.png]]
Il ciclo scelto contiene quindi i seguenti archi: $\{ \{i, c\}, \{c, d\}, \{c,f\}, \{d, f\}, \{f, g\}\}$. Tra questi l'arco con peso massimo corrisponde a $\{d, f\}$, con peso pari a $14$. Coloriamolo quindi di rosso.
Proseguiamo ora applicando nuovamente la regola del taglio, tagliando a piacere e rispettando gli archi blu (**quelli rossi possiamo anche renderli attraversanti**):![[Pasted image 20250614111522.png]]
Ora selezioniamo l'arco non-colorato con peso minimo tra quelli attraversanti il taglio, ovvero $\{g, f\}$. Coloriamolo di blu e procediamo.
Andiamo avanti così fino a raggiungere uno scenario dove tutti gli archi sono colorati:![[Pasted image 20250614111846.png]]
Questo sarà uno dei possibili $\text{MST}$, in quanto rispetta tutte e tre le seguenti condizioni necessarie per un $\text{MST}$ valido:
- collega tutti i vertici di $G$;
- in esso si ha assenza di cicli;
- avendo $n$ vertici si hanno $n - 1$ archi blu: in questo caso $9$ vertici e $8$ archi blu.
### Algoritmo di Kruskal
Questo algoritmo crea un insieme $A$ (il nostro futuro $\text{MST}$), per poi ordinare tutti gli archi del grafo $G$ in modo crescente e scorrere su di essi:
- se l'arco attuale non forma un ciclo con gli altri archi già in $A$, aggiungilo (coloralo di blu);
- altrimenti, (scartalo) coloralo di rosso;

ripetendo questo loop fino ad avere $n-1$ archi in $A$.
#### Esempio di Applicazione dell'Algoritmo di Kruskal
Avendo un grafo $G$ come il seguente:
![[Pasted image 20250616162644.png]]
anzitutto ordiniamo in modo crescente tutti gli archi, sulla base del loro peso. In seguito cominciamo ad iterare su questi: il primo elemento sarà l'arco con peso $1$, che ovviamente essendo il primo non formerà archi. Inseriamolo in $A$:
![[Pasted image 20250616162758.png]]
Ora procediamo e troviamo un arco con peso $2$: anche questo, essendo solamente il secondo, non formerà cicli. Inseriamolo in $A$:
![[Pasted image 20250616162849.png]]
Ora procediamo così per ogni arco:
![[Pasted image 20250616162912.png]]
![[Pasted image 20250616162924.png]]
![[Pasted image 20250616162937.png]]
A questo punto ci troveremo di fronte ad un arco con peso pari a $6$: questo tuttavia creerebbe un ciclo. Non inseriamolo in $A$ e marchiamolo di rosso, per poi proseguire:
![[Pasted image 20250616162953.png]]
![[Pasted image 20250616163008.png]]
![[Pasted image 20250616163108.png]]
![[Pasted image 20250616163117.png]]
![[Pasted image 20250616163126.png]]
![[Pasted image 20250616163135.png]]
![[Pasted image 20250616163145.png]]
![[Pasted image 20250616163154.png]]
#### Implementare l'Algoritmo di Kruskal
La parte "difficile" dell'algoritmo di $\text{Kruskal}$ sta nel capire se un arco crei un ciclo in un albero oppure no: per farlo serviamoci delle [[Strutture dati (old)#Strutture Union-Find|Strutture UF]]. Ci basta difatti lavorare sugli insiemi contenenti gli archi (identificati da una lettera) per capire se un eventuale aggiunta porterebbe ad un ciclo!
```pseudocodice
function Kruskal(Grafo G)
	let UF be a new UnionFind
	let T be a new Tree
	// sort degli archi ipotizzando costo O(n log n)
	for {u, v} in G
		Tu = UF.find(u)
		Tv = UF.find(v)
		if Tu != Tv
			T = T.Insert({u, v})
			UF.union(Tu, Tv)
	return T
```
Con costo totale $\cal O(E \cdot \log{E})$, dove $\cal E =$ numero di archi in $G$.
### Algoritmo di Prim
Questo algoritmo usa solamente la regola del Taglio partendo da un nodo identificato come $\text{radice}$ e chiamato $r.$ Ad ogni passo verrà quindi aggiunto l'arco di peso minimo che collega un nodo già raggiunto di un albero con uno ancora non raggiunto.
#### Implementazione dell'Algoritmo di Prim
Per implementare questo algoritmo si userà una [[Strutture dati (old)#Priority Queue|Priority Queue]] $Q$ per contenere tutti i nodi visitati ma non ancora inseriti all'interno dell'$\text{MST}$. Un nodo verrà rimosso da $Q$ solamente dopo essere stato inserito e questo loop terminerà quando $Q$ si svuoterà.
```pseudocodice
function Prim(Grafo G=<V, E, w>, nodo s)
	for each v in G.V
		v.dist = -1
		v.parent = NIL
		v.isInT = false // controlliamo che non sia inserito nell'MST
	s.dist = 0 // radice scelta
	let Q be a new CodaPriorita
	Q.insert(s, s.dist)
	while !Q.isEmpty()
		u = Q.find()
		Q.deleteMin()
		u.isInT = true
		for each v in u.adj
			if v.dist == -1
				Q.insert(v, w(u, v))
				v.dist = w(u, v)
				v.parent = u
			else if w(u, v) < v.dist
				Q.decreaseKey(v, w(u, v))
				v.dist = w(u, v)
				v.parent = u
```
Con costo computazionale $\cal O(E \log{n})$.
#### Esempio di Applicazione dell'Algoritmo di Prim
Supponendo di avere un grafo $G$ come il seguente:
![[Pasted image 20250616171317.png]]
Prendiamo come radice il nodo $a$ e tagliamo in questa maniera:
![[Pasted image 20250616171335.png]]
Ora inseriamo i nodi $b$ ed $h$ all'interno di $Q$, con associato il peso dell'arco collegante ad $a:$
![[Pasted image 20250616171415.png]]
Scegliamo ora tra i due il nodo con peso minore, ovvero $b$. Cancelliamolo da $Q$ e coloriamo di blu l'arco che lo collega ad $a:$
![[Pasted image 20250616171458.png]]
Ripetiamo per i nodi adiacenti al nodo appena inserito (ovvero per $c$), tenendo nella coda *anche* quelli precedentemente visitati e non inseriti (ovvero $h$)$:$
![[Pasted image 20250616171618.png]]
![[Pasted image 20250616171627.png]]
![[Pasted image 20250616171639.png]]
![[Pasted image 20250616171647.png]]
![[Pasted image 20250616171656.png]]
![[Pasted image 20250616171703.png]]
![[Pasted image 20250616171711.png]]
![[Pasted image 20250616171720.png]]
![[Pasted image 20250616171728.png]]
![[Pasted image 20250616171735.png]]
![[Pasted image 20250616171743.png]]
![[Pasted image 20250616171752.png]]
![[Pasted image 20250616171805.png]]
![[Pasted image 20250616171812.png]]
### Cammini a Costo Minimo
Un cammino a Costo Minimo corrisponde alla distanza minima percorribile per connettere tra loro dei nodi di un grafo.
> **N.B.** In un grafo orientato $G$ *non* si ha un cammino minimo quando è presente un ciclo con costo negativo.

Esistono  $3$ tipi di problemi inerenti alla ricerca di un percorso minimo:
1. cammino a distanza minima tra due nodi $u$ e $v;$
2. single-source shortest path, ovvero la distanza tra un nodo $s$ e tutti i nodi da esso raggiungibili;
3. all-pairs shortest paths, ovvero le distanze tra ogni coppia di nodi $u$ e $v$.
#### Condizione di Bellman
In un grafo orientato pesato la distanza minima tra due punti $u$ e $v$ corrisponde al costo di un cammino minimo che li connette. Da questa affermazione possiamo continuare dicendo che:
$$d_{sv} \leq d_{su} + w(u,v) \ \forall \ s \in V, (u, v) \in E$$da cui deduciamo che l'arco $(u,v)$ faccia parte di un cammino di costo minimo se e solo se:$$d_{sv} = d_{su} + w(u,v)$$
#### Tecnica del Rilassamento
Per trovare un cammino minimo potremmo mantenere una stima $D_{sv} \geq d_{sv}$ della lunghezza del cammino di costo minimo tra $s$ e $v$, effettuando ad ogni step un **rilassamento**, riducendo così progressivamente la stima fino ad arrivare ad avere $D_{sv} = d_{sv}$.
Quando si parla di rilassamento si intende quindi un'operazione volta a migliorare la stima delle distanza tra due nodi, migliorando così la conoscenza del cammino minimo.
Un esempio di rilassamento potrebbe essere:
```pseudocodice
if (D(s, u) + w(u, v) < D(s, v))
	D(s, v) = D(s, u) + w(u, v)
```
#### Algoritmo di Bellman-Ford
Questo algoritmo è del tipo **single-source shortest path**.
Considerando un cammino di costo minimo inizialmente ignoto $\pi^\times_{s \ vk} = (s, v_1, \dots, v_k)$, sapendo che $d_{sv_k} = d_{sv_{k-1}} + w(v_{k-1}, v_k)$ e partendo da $D_{ss} = 0, D_{st} = \infty$ per $t \not= s$, potremmo effettuare i seguenti step di rilassamento:
$$\begin{array}{c}D_{sv_1} = D_{ss} + w(s, v_1) \\ D_{sv_2} = D_{sv_1} + w(v_1, v_2) \\ \dots \\ D_{sv_k} = D_{sv_{k - 1}} + w(v_{k-1}, v_k) \end{array}$$Il problema di questo approccio consiste nel fatto che non conoscendo né gli archi del cammino minimo né il loro ordine, non sarebbe possibile effettuare i rilassamenti nell'ordine corretto.
Per ovviare a questo problema sarà sufficiente effettuare questa operazione per tutti gli archi del grafo, in quanto così facendo includeremmo con certezza anche il passo corretto.
Questo significa che dopo $n - 1$ iterazioni, con $n$ pari al numero di possibili destinazioni partendo da un certo nodo, si saranno effettuati tutti i rilassamenti corretti.
##### Implementazione dell'Algoritmo Bellman-Ford
```pseudocodice
function BellmanFord(Grafo G = (V, E, w), int s) -> double[1, ..., n]
	int n = G.numNodi()
	int pred[1, ..., n], v, u;
	double D[1, ..., n];
	for i = 1, ..., n
		D[i] = infinito;
		pred[i] = -1;
	D[s] = 0;
	for i = 1, ..., n - 1
		for each (u, v) in E
			if (D[u] + w(u, v) < D[v])
				D[v] = D[u] + w(u, v);
				pred[v] = u;
	\\ controllo per cicli negativi
	for each (u, v) in E
		if (D[u] + w(u, w) < D[v])
			error "Il grafo contiene cicli negativi"
	return D;
```
Questo algoritmo funziona anche con pesi negativi, ma non con cicli negativi.
Tuttavia nel caso in cui tutti i pesi siano *non-negativi* esiste un algoritmo più efficiente, ovvero quello di Dijkstra.
##### Esempio di Utilizzo dell'Algoritmo di Bellman-Ford
![[Pasted image 20250703112156.png]]
![[Pasted image 20250703112221.png]]
![[Pasted image 20250703112230.png]]
![[Pasted image 20250703112241.png]]
![[Pasted image 20250703112251.png]]
![[Pasted image 20250703112301.png]]
#### Algoritmo di Dijkstra
Questo algoritmo si basa sul seguente **Lemma**: avendo un grafo orientato $G$ all'interno del quale tutti i pesi sono $\geq 0$, ed una parte $T$ dell'albero dei cammini di costo minimo radicato in $s$, allora l'arco $(u, v)$ con $u \in V(T)$ e $v \not \in V(T)$ che minimizza la quantità $d_{su} + w(u, v)$ appartiene ad un cammino minimo da $s$ a $v$.
##### Implementazione dell'Algoritmo di Dijkstra
```pseudocodice
function Dijkstra(Grafo G=(V, E, w), int s) -> double [1, ..., n]
	int n = G.numNodi()
	int pred = [1, ..., n], u, v;
	double D[1, ..., n];
	for v = 1, ..., n
		D[v] = infinito;
		pred[v] = -1;
	D[s] = 0;
	CodaConPriorita<int, double> Q;
	Q.insert(s, D[s]);
	while (!Q.isEmpty())
		u = Q.find();
		Q.deleteMin(); // rimuove nodo con distanza minima
		for each v in u.adiacents()
			if (D[v] == infinito)
				D[v] = D[u] + w(u, v);
				Q.insert(v, D[v]);
				pred[v] = u;
			else if (D[u] + w(u, v) < D[v])
				Q.decreaseKey(v, D[u] + w(u, v));
				D[v] = D[u] + w(u, v); // la nuova distanza di v da s
				pred[v] = u;
	return D;
```
Il costo di questo algoritmo dipende dai costi di [[Strutture dati (old)#Priority Queue#Trovare l'elemento con chiave maggiore (o minore)|find]], [[Strutture dati (old)#Priority Queue#Cancellare l'elemento con chiave maggiore (o minore)|deleteMin]], [[Strutture dati (old)#Priority Queue#Inserire un elemento con una chiave associata|insert]] e di [[Strutture dati (old)#Priority Queue#Aumentare (o diminuire) la chiave di un elemento|decreaseKey]], tutte con costo $\cal O(\log {n})$. Le prime due verranno eseguite al massimo $n$ volte, mentre le altre due $m$ volte (una per arco). Il costo complessivo della funzione sarà quindi $\cal O((n + m) \log {n})$.
### Algoritmo di Floyd e Warshall
Questo algoritmo è del tipo **all-pairs shortests paths**, è applicabile anche a grafi contenenti pesi negativi (basta non ci siano *cicli* negativi) ed è basato sulla programmazione dinamica.
Avendo $D_{xy}^k$ la distanza minima tra i nodi $x$ ed $y$, supponendo che tutti i nodi intermedi possano appartenere solamente all'insieme $\{1, \dots, k\}$.
Seguendo la precedente definizione si può definire $D_{xy}^0$ come la distanza tra due nodi, senza passare per nodi intermedi. Ovvero:$$D_{xy}^0 = \begin{cases}0 & se \ x = y \\ w(x, y) & se \ (x, y) \in E \\ \infty & se \ (x, y) \not\in E\end{cases}$$In generale si può invece dire che, volendo arrivare da $x$ ad $y$ senza mai passare per il $k$-esimo nodo, la distanza sarà $D_{xy}^{k-1}$. Se si volesse passare invece per quest'ultimo, si avrebbe una distanza pari a $D_{xy}^{k-1} + D_{xy}^{k-1}$.
#### Implementazione dell'Algoritmo di Floyd e Warshall
```pseudocodice
function FloydWarshall(Grafo G=(V,E,w)) -> double[1, ..., n; 1, ..., n;]
	int n = G.numNodi();
	double D[1, ..., n; 1, ..., n;];
	int x, y, k, next[1, ..., n; 1, ..., n];
	for x = 1, ..., n
		for y = 1, ..., n
			if (x == y)
				D[x, y] = 0;
				next[x, y] = -1;
			else if ((x, y) in E)
				D[x, y] = w(x, y);
				next[x, y] = y;
			else
				D[x, y] = infinito;
				next[x, y] = -1;
	for k = 1, ..., n
		for x = 1, ..., n
			for y = 1, ..., n
				D[x, y, k] = D[x, y, k - 1];
				if (D[x, k] + D[k, y] < D[x, y])
					D[x, y] = D[x, k] + D[k, y];
					next[x, y] = next[x, k];
	for x = 1, ..., n
		if (D[x, x] < 0)
			error "Il grafo contiene cicli negativi";
	return D[1, ..., n; 1, ..., n];
```
Con costo pari ad $\cal O(n^3)$.